{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:02:19.179009Z",
     "iopub.status.busy": "2022-07-27T01:02:19.178115Z",
     "iopub.status.idle": "2022-07-27T01:02:19.184491Z",
     "shell.execute_reply": "2022-07-27T01:02:19.183756Z",
     "shell.execute_reply.started": "2022-07-27T01:02:19.178971Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cd to your workstation, if necessary\n",
    "%cd /home/aistudio/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:02:19.186267Z",
     "iopub.status.busy": "2022-07-27T01:02:19.185848Z",
     "iopub.status.idle": "2022-07-27T01:02:23.924777Z",
     "shell.execute_reply": "2022-07-27T01:02:23.923732Z",
     "shell.execute_reply.started": "2022-07-27T01:02:19.186244Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用pandas读取数据集\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_table('data/Chinese/train.csv', sep='\\t')  # 训练集\n",
    "test = pd.read_table('data/Chinese/test.csv', sep='\\t')    # 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:02:23.926395Z",
     "iopub.status.busy": "2022-07-27T01:02:23.926004Z",
     "iopub.status.idle": "2022-07-27T01:02:23.966988Z",
     "shell.execute_reply": "2022-07-27T01:02:23.966296Z",
     "shell.execute_reply.started": "2022-07-27T01:02:23.926369Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:02:23.968289Z",
     "iopub.status.busy": "2022-07-27T01:02:23.968018Z",
     "iopub.status.idle": "2022-07-27T01:02:25.846555Z",
     "shell.execute_reply": "2022-07-27T01:02:25.845527Z",
     "shell.execute_reply.started": "2022-07-27T01:02:23.968265Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入所需的第三方库\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import collections\n",
    "from functools import partial\n",
    "import random\n",
    "import time\n",
    "import inspect\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import IterableDataset\n",
    "from paddle.utils.download import get_path_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:02:25.849604Z",
     "iopub.status.busy": "2022-07-27T01:02:25.848959Z",
     "iopub.status.idle": "2022-07-27T01:02:34.975468Z",
     "shell.execute_reply": "2022-07-27T01:02:34.974375Z",
     "shell.execute_reply.started": "2022-07-27T01:02:25.849570Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:02:34.977652Z",
     "iopub.status.busy": "2022-07-27T01:02:34.976959Z",
     "iopub.status.idle": "2022-07-27T01:02:35.410967Z",
     "shell.execute_reply": "2022-07-27T01:02:35.410078Z",
     "shell.execute_reply.started": "2022-07-27T01:02:34.977621Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入paddlenlp所需的相关包\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.dataset.common import md5file\n",
    "from paddlenlp.datasets import DatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:02:35.412878Z",
     "iopub.status.busy": "2022-07-27T01:02:35.412135Z",
     "iopub.status.idle": "2022-07-27T01:03:14.468451Z",
     "shell.execute_reply": "2022-07-27T01:03:14.467672Z",
     "shell.execute_reply.started": "2022-07-27T01:02:35.412847Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用roberta-wwm-ext-large模型\n",
    "MODEL_NAME_OR_PATH = \"roberta-wwm-ext-large\"\n",
    "# 从保存的参数中读取\n",
    "# MODEL_NAME_OR_PATH = 'checkpoint'\n",
    "# 只需指定想要使用的模型名称和文本分类的类别数即可完成Fine-tune网络定义，通过在预训练模型后拼接上一个全连接网络（Full Connected）进行分类\n",
    "model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(MODEL_NAME_OR_PATH, num_classes=8) # 此次分类任务为8分类任务，故num_classes设置为8\n",
    "# 定义模型对应的tokenizer，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。需注意tokenizer类要与选择的模型相对应，具体可以查看PaddleNLP相关文档\n",
    "tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:14.470414Z",
     "iopub.status.busy": "2022-07-27T01:03:14.469863Z",
     "iopub.status.idle": "2022-07-27T01:03:14.494975Z",
     "shell.execute_reply": "2022-07-27T01:03:14.494052Z",
     "shell.execute_reply.started": "2022-07-27T01:03:14.470370Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义要进行分类的8个类别\n",
    "label_list=list(train.label.unique())\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:14.496975Z",
     "iopub.status.busy": "2022-07-27T01:03:14.496499Z",
     "iopub.status.idle": "2022-07-27T01:03:14.748939Z",
     "shell.execute_reply": "2022-07-27T01:03:14.748071Z",
     "shell.execute_reply.started": "2022-07-27T01:03:14.496934Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义数据集对应文件及其文件存储格式\n",
    "class NewsData(DatasetBuilder):\n",
    "    SPLITS = {\n",
    "        'train': 'data/Chinese/train.csv',  # 训练集\n",
    "        'dev': 'data/Chinese/test.csv',      # 验证集\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        filename = self.SPLITS[mode]\n",
    "        return filename\n",
    "\n",
    "    def _read(self, filename):\n",
    "        \"\"\"读取数据\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            head = None\n",
    "            for line in f:\n",
    "                data = line.strip().split(\"\\t\")    # 以'\\t'分隔各列\n",
    "                if not head:\n",
    "                    head = data\n",
    "                else:\n",
    "                    try:\n",
    "                        text_a, label = data\n",
    "                        yield {\"text_a\": text_a, \"label\": label}  # 此次设置数据的格式为：text_a,label，可以根据具体情况进行修改\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "    def get_labels(self):\n",
    "        return label_list   # 类别标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:14.750375Z",
     "iopub.status.busy": "2022-07-27T01:03:14.750023Z",
     "iopub.status.idle": "2022-07-27T01:03:15.048208Z",
     "shell.execute_reply": "2022-07-27T01:03:15.047194Z",
     "shell.execute_reply.started": "2022-07-27T01:03:14.750350Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义数据集加载函数\n",
    "def load_dataset(name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = NewsData  # 加载定义的数据集格式\n",
    "    print(reader_cls)\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:15.049872Z",
     "iopub.status.busy": "2022-07-27T01:03:15.049482Z",
     "iopub.status.idle": "2022-07-27T01:03:17.393889Z",
     "shell.execute_reply": "2022-07-27T01:03:17.392960Z",
     "shell.execute_reply.started": "2022-07-27T01:03:15.049845Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 加载训练和验证集\n",
    "train_ds, dev_ds = load_dataset(splits=[\"train\", \"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:17.395479Z",
     "iopub.status.busy": "2022-07-27T01:03:17.395094Z",
     "iopub.status.idle": "2022-07-27T01:03:17.402657Z",
     "shell.execute_reply": "2022-07-27T01:03:17.402000Z",
     "shell.execute_reply.started": "2022-07-27T01:03:17.395452Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义数据加载和处理函数\n",
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "    qtconcat = example[\"text_a\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)  # tokenizer处理为模型可接受的格式 \n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# 定义数据加载函数dataloader\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # 训练数据集随机打乱，测试数据集不打乱\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:17.406025Z",
     "iopub.status.busy": "2022-07-27T01:03:17.405572Z",
     "iopub.status.idle": "2022-07-27T01:03:17.409108Z",
     "shell.execute_reply": "2022-07-27T01:03:17.408498Z",
     "shell.execute_reply.started": "2022-07-27T01:03:17.406002Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 参数设置：\n",
    "# 批处理大小，显存如若不足的话可以适当改小该值  \n",
    "batch_size = 32\n",
    "# 文本序列最大截断长度，需要根据文本具体长度进行确定，最长不超过512。 \n",
    "max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:17.410416Z",
     "iopub.status.busy": "2022-07-27T01:03:17.409962Z",
     "iopub.status.idle": "2022-07-27T01:03:17.415665Z",
     "shell.execute_reply": "2022-07-27T01:03:17.414993Z",
     "shell.execute_reply.started": "2022-07-27T01:03:17.410394Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "# 训练集迭代器\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "# 验证集迭代器\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:17.416938Z",
     "iopub.status.busy": "2022-07-27T01:03:17.416508Z",
     "iopub.status.idle": "2022-07-27T01:03:17.423956Z",
     "shell.execute_reply": "2022-07-27T01:03:17.423299Z",
     "shell.execute_reply.started": "2022-07-27T01:03:17.416916Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义超参，loss，优化器等\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 定义训练配置参数：\n",
    "# 定义训练过程中的最大学习率\n",
    "learning_rate = 4e-5\n",
    "# 训练轮次\n",
    "epochs = 1\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.0\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "# AdamW优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "metric = paddle.metric.Accuracy()              # accuracy评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 加载学习器和优化器\n",
    "if os.path.isfile(MODEL_NAME_OR_PATH):\n",
    "    try:\n",
    "        lr_scheduler.set_state_dict(paddle.load(\"lr\"))\n",
    "        optimizer.set_state_dict(paddle.load(\"opt\"))\n",
    "        print('Learning rate:', optimizer.get_lr())\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    print(\"Loaded parameters from %s\" % MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:17.425219Z",
     "iopub.status.busy": "2022-07-27T01:03:17.424780Z",
     "iopub.status.idle": "2022-07-27T01:03:17.430575Z",
     "shell.execute_reply": "2022-07-27T01:03:17.429956Z",
     "shell.execute_reply.started": "2022-07-27T01:03:17.425197Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义模型训练验证评估函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))  # 输出验证集上评估效果\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu  # 返回准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:17.431803Z",
     "iopub.status.busy": "2022-07-27T01:03:17.431360Z",
     "iopub.status.idle": "2022-07-27T01:03:17.437435Z",
     "shell.execute_reply": "2022-07-27T01:03:17.436831Z",
     "shell.execute_reply.started": "2022-07-27T01:03:17.431782Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 固定随机种子便于结果的复现\n",
    "seed = 1024\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "paddle.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T01:03:17.438660Z",
     "iopub.status.busy": "2022-07-27T01:03:17.438249Z",
     "iopub.status.idle": "2022-07-27T07:30:19.818315Z",
     "shell.execute_reply": "2022-07-27T07:30:19.816055Z",
     "shell.execute_reply.started": "2022-07-27T01:03:17.438638Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型训练：\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "save_dir = \"checkpoint\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "pre_accu=0\n",
    "accu=0\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "    save_param_path = os.path.join(save_dir, 'model_state.pdparams')  # 保存模型参数\n",
    "    save_lr_path = os.path.join(save_dir, 'lr')\n",
    "    save_opt_path = os.path.join(save_dir, 'opt')\n",
    "\n",
    "    # paddle.save(model.state_dict(), save_param_path)\n",
    "    model.save_pretrained(save_dir)\n",
    "    paddle.save(lr_scheduler.state_dict(), save_lr_path)\n",
    "    paddle.save(optimizer.state_dict(), save_opt_path)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "    accu = evaluate(model, criterion, metric, dev_data_loader)\n",
    "    print(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-27T07:30:19.819416Z",
     "iopub.status.idle": "2022-07-27T07:30:19.819906Z",
     "shell.execute_reply": "2022-07-27T07:30:19.819705Z",
     "shell.execute_reply.started": "2022-07-27T07:30:19.819685Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存label_map\n",
    "label_map = {}\n",
    "for idx, value in enumerate(label_list):\n",
    "    label_map[idx] = value\n",
    "\n",
    "with open(save_dir + '/label_map.json', 'w') as f:\n",
    "    json.dump(label_map, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
